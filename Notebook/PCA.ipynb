{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 17 09:47:02 2018\n",
    "\n",
    "@author: shahelkhan\n",
    "\n",
    "PCA.py\n",
    "\n",
    "Purpose:\n",
    "    - Use trainingdataset to reduce the number of dimensions in the problem\n",
    "    - Transform the full dataset onto these reduced dimensions\n",
    "    - Store the PCA object and the information about the eigenvectors it has created\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from sklearn import decomposition\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import Print\n",
    "\n",
    "start_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create(address, run, n_dimen):\n",
    "    print(\"PCA.create\")\n",
    "    \"\"\" This function takes the training dataset and creates the PCA object,\n",
    "    whilst returning the transfromed dataset \"\"\"\n",
    "    \n",
    "    # Load depth\n",
    "    depth = None\n",
    "    depth = Print.readDepth(address, run)\n",
    "    \n",
    "    # Load Training data\n",
    "    lon_train, lat_train, Tint_train_array, X_train_array, \\\n",
    "            Sint_train_array, varTime_train = None, None, None, None, None, None\n",
    "    lon_train, lat_train, Tint_train_array, X_train_array, \\\n",
    "            Sint_train_array, varTime_train = Print.readLoadFromFile_Train(address, run, depth)\n",
    "    \n",
    "    # Start the PCA process\n",
    "    pca, pca_store, X_pca_train, variance_sum = None, None, None, None\n",
    "    pca, pca_store, X_pca_train, variance_sum  = PrincipleComponentAnalysis( address, run, X_train_array, n_dimen)\n",
    "    col_reduced = np.size(X_pca_train,1)    # Variable retains the number of reduced dimensions\n",
    "    \n",
    "    \"\"\" Now we can print the reduced training dataset to a file \"\"\"\n",
    "#    print(\"Starting Print PCA\")\n",
    "# NOTE: I'm excluding Tint and Sint at this point in the code\n",
    "    Print.printPCAToFile_Train(address, run, lon_train, lat_train, \\\n",
    "                                X_pca_train, varTime_train, col_reduced)\n",
    "    Print.printColreduced(address, run, col_reduced)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(address, run):\n",
    "    print(\"PCA.apply\")\n",
    "    # Load depth\n",
    "    depth = None\n",
    "    depth = Print.readDepth(address, run)\n",
    "    \n",
    "    # Load col_reduced value\n",
    "    col_reduced = None\n",
    "    col_reduced = Print.readColreduced(address, run)\n",
    "    \n",
    "    # Load full data array - X\n",
    "    lon, lat, Tint_array, X_array, \\\n",
    "            Sint_array, varTime = None, None, None, None, None, None\n",
    "    lon, lat, Tint_array, X_array, Sint_array, varTime  = \\\n",
    "            Print.readLoadFromFile(address, run, depth)\n",
    "            \n",
    "    # Load PCA object\n",
    "    pca = None\n",
    "    with open(address+'Objects/PCA_object.pkl', 'rb') as input:\n",
    "        pca = pickle.load(input)\n",
    "    \n",
    "    # tansform X to X_pca\n",
    "    X_pca = None\n",
    "    X_pca = pca.transform(X_array)    \n",
    "    \n",
    "    # Print X_pca to file\n",
    "    Print.printPCAToFile(address, run, lon, lat, X_pca, varTime, col_reduced)  \n",
    "    del pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrincipleComponentAnalysis( address, run, X_train, n_dimen):\n",
    "    print(\"PCA.PrincipleComponentAnalysis\")\n",
    "    \"\"\" This function initialises the PCA object and is called in create() \"\"\"\n",
    "    pca = decomposition.PCA(n_components = n_dimen)     # Initialise PCA object\n",
    "    pca.fit(X_train)    # Fit the PCA to the training data\n",
    "    X_pca_train = pca.transform(X_train)    # transform the Training Data set to reduced space\n",
    "    variance_sum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    pca_store = address+\"Objects/PCA_object.pkl\"    \n",
    "    with open(pca_store, 'wb') as output:\n",
    "        pcaObject = pca\n",
    "        pickle.dump(pcaObject, output, pickle.HIGHEST_PROTOCOL)\n",
    "    del pcaObject    \n",
    "    \n",
    "    return pca, pca_store, X_pca_train, variance_sum\n",
    "\n",
    "print('PCA runtime = ', time.clock() - start_time,' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
