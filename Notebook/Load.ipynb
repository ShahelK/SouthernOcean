{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahelkhan/anaconda2/lib/python2.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Print.ipynb\n",
      "('Printing runtime = ', 0.6693320000000003, ' s')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Aug 16 13:08:09 2018\n",
    "\n",
    "@author: shahelkhan\n",
    "\n",
    "Load.py\n",
    "\n",
    "Purpose:\n",
    "    - Load the data\n",
    "    - Clean (remove Nan values from profiles and depths)\n",
    "    - Centre and Standardise\n",
    "    - Print data to store the results\n",
    "\n",
    "\"\"\"\n",
    "# Importing modules\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "import Print\n",
    "\n",
    "start_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(address, filename_raw_data, run, subsample_uniform, subsample_random,\\\n",
    "         subsample_inTime, grid, conc, fraction_train, inTime_start, inTime_finish,\\\n",
    "         fraction_nan_samples, fraction_nan_depths, run_Bic=False):\n",
    "    print(\"Starting Load.main\")\n",
    "    \"\"\" Main function for module\"\"\"\n",
    "    lon, lat, Tint, Sint, varTime = load(filename_raw_data)\n",
    "    Tint, Sint, depth = removeDepthFractionNan(Tint, Sint, fraction_nan_depths)\n",
    "    lon, lat, Tint, Sint, varTime = removeSampleFractionNan(lon, lat, Tint, Sint, varTime, fraction_nan_samples)\n",
    "    Tint, Sint = dealwithNan(Tint, Sint)\n",
    "    \n",
    "    ## At this point the data has been successfully cleaned.\n",
    "    \"\"\" Now I need to subselect the training data \"\"\"\n",
    "    Tint_train, Sint_train, varTime_train = None, None, None\n",
    "    if subsample_uniform: # Currently the only option working \n",
    "        lon_train, lat_train, Tint_train, Sint_train, varTime_train = uniformTrain(lon, lat, Tint, Sint, varTime, depth, grid, conc)\n",
    "    if subsample_random: # Now also written and working\n",
    "        lon_train, lat_train, Tint_train, Sint_train, varTime_train = randomTrain(lon, lat, Tint, Sint, varTime, depth, fraction_train)\n",
    "    if subsample_inTime:\n",
    "        lon_train, lat_train, Tint_train, Sint_train, varTime_train = inTimeTrain(lon, lat, Tint, Sint, varTime, depth, inTime_start, inTime_finish)\n",
    "    \n",
    "    ## At this point we should have a training data set to go with the full data set\n",
    "    \"\"\" Now we can centre and standardise the training data, and the whole data will follow \"\"\"\n",
    "    # NOTE: currently unsure how to include Sint in the centring\n",
    "    # I also change the nomenaculature at this point in the code\n",
    "    \"\"\" It is important that the training data set initialises the standardised object !! \"\"\"\n",
    "    stand, stand_store, varTrain_centre = centreAndStandardise(address, run, Tint_train)\n",
    "    var_centre = stand.transform(Tint)  # Centre the full dataset based on the training data set\n",
    "    \n",
    "    \"\"\" Create the test dataset, by subtracting the set(var_train) from set(var)\"\"\"\n",
    "    \"\"\"\n",
    "    lon_test = [x for x in lon if x not in set(lon_train)]\n",
    "    lat_test = [x for x in lat if x not in set(lat_train)]\n",
    "    Tint_test = [x for x in Tint if x not in set(Tint_train)]\n",
    "    Sint_test = [x for x in Sint if x not in set(Sint_train)]\n",
    "    varTime_test = [x for x in varTime if x not in set(varTime_train)]\n",
    "    \n",
    "    varTest_centre = stand.transform(Tint_test)\n",
    "    \"\"\"\n",
    "    # INFORMATION\n",
    "    # varTrain_centre stores the training, standardised\n",
    "    # var_centre stores the full standardised data set\n",
    "    # varTest_centre stores the test dataset\n",
    "    \n",
    "    \"\"\" Now we can print the results of this process to a file for later use \"\"\"\n",
    "#    print(\"Starting Print\")\n",
    "    print(\"varTrain_centre.shape = \", varTrain_centre.shape)\n",
    "    if not run_Bic:\n",
    "        Print.printLoadToFile(address, run, lon, lat, Tint, var_centre, Sint, varTime, \\\n",
    "                              depth)\n",
    "        Print.printLoadToFile_Train(address, run, lon_train, lat_train, Tint_train, \\\n",
    "                                varTrain_centre, Sint_train, varTime_train, depth)\n",
    "        #Print.printLoadToFile_Test(address, run, lon_test, lat_test, Tint_test, \\\n",
    "        #                        varTest_centre, Sint_test, varTime_test, depth)\n",
    "        Print.printDepth(address, run, depth)\n",
    "    \n",
    "    if run_Bic:\n",
    "        return lon_train, lat_train, Tint_train, varTrain_centre, Sint_train, varTime_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions which Main uses\n",
    "def load(filename_raw_data):\n",
    "    print(\"Load.load\")\n",
    "    \"\"\" This function loads the raw data from a .mat file \"\"\"\n",
    "    lon, lat, Tint, Sint, varTime = [], [], [], [], []\n",
    "    # Import lon, lat, Temperature, Salinity and times explicitly\n",
    "    variables = ['lon', 'lat', 'Tint', 'Sint','dectime']\n",
    "    mat = h5py.File(filename_raw_data)\n",
    "    \n",
    "    lon = mat[\"lon\"]\n",
    "    lon = np.array(lon)[:,0]\n",
    "    lat = mat[\"lat\"]\n",
    "    lat = np.array(lat)[:,0]\n",
    "    Tint = mat[\"Tint\"]\n",
    "    Tint = np.array(Tint)\n",
    "    Sint = mat[\"Sint\"]\n",
    "    Sint = np.array(Sint)\n",
    "    varTime = mat[\"dectime\"]\n",
    "    varTime = np.array(varTime)\n",
    "    \n",
    "    print(\"Shape of variable = \", Tint.shape)\n",
    "#    print(\"axis 0 = \", np.ma.size(Tint, axis=0)) # axis 0 should be ~290520\n",
    "#    print(\"axis 1 = \", np.ma.size(Tint, axis=1)) # axis 1 should be ~400\n",
    "    \n",
    "    return lon, lat, Tint, Sint, varTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeDepthFractionNan(VAR, VAR2, fraction_of):\n",
    "    print(\"Load.removeDepthFractionNan\")\n",
    "    \"\"\" This function removes all depths will a given number of Nan values \"\"\"\n",
    "    delete_depth = []\n",
    "    depth_remain = 5 * np.arange(np.ma.size(VAR, axis=1)) # Create a pressure array to record the pressure of the depths kept in data\n",
    "    \n",
    "    for i in range(np.ma.size(VAR, axis=1)):\n",
    "        var_mean_nan = []\n",
    "        var_nan_sum = 0\n",
    "        var_mean_nan = np.isnan(VAR[:,i])\n",
    "        var_nan_sum = var_mean_nan.sum()\n",
    "        if var_nan_sum >= np.ma.size(VAR, axis=0)/fraction_of:\n",
    "            delete_depth.append(i)\n",
    "            #print(i,\" DELETED\")\n",
    "            \n",
    "    delete_depth = np.squeeze(np.asarray(delete_depth))\n",
    "    #print(delete_depth.shape)\n",
    "    VAR = np.delete(VAR, (delete_depth), axis=1 )\n",
    "    VAR2 = np.delete(VAR2, (delete_depth), axis=1 )\n",
    "    depth_remain = np.delete(depth_remain, (delete_depth))\n",
    "    print(\"Number of depths deleted above the 1/\"+str(fraction_of)+\" criterion = \", delete_depth.size)\n",
    "    #print(\"Depth pressure remain = \", depth_remain.shape)\n",
    "    #print(\"VAR shape after half Sample removed = \", VAR.shape)\n",
    "    return VAR, VAR2, depth_remain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeSampleFractionNan(LON, LAT, VAR, VAR2, varTime, fraction_of):\n",
    "    print(\"Load.removeSampleFractionNan\")\n",
    "    \"\"\" This function removes all profiles will a given number of Nan values \"\"\"\n",
    "    delete_sample = []\n",
    "    sample_count = 0\n",
    "    for i in range(np.ma.size(VAR, axis=0)):\n",
    "        var_mean_nan = []\n",
    "        var_mean_nan = np.isnan(VAR[i,:])\n",
    "        var_nan_sum = var_mean_nan.sum()        \n",
    "        if var_nan_sum >= np.ma.size(VAR, axis=1)/fraction_of:\n",
    "            sample_count = sample_count + 1\n",
    "            delete_sample.append(i)\n",
    "            \n",
    "    delete_sample = np.squeeze(np.asarray(delete_sample))\n",
    "    #print(delete_sample.shape)\n",
    "    VAR = np.delete(VAR, (delete_sample), axis=0 )\n",
    "    VAR2 = np.delete(VAR2, (delete_sample), axis=0 )\n",
    "    varTime = np.delete(varTime, (delete_sample))\n",
    "    LON = np.delete(LON, (delete_sample))\n",
    "    LAT = np.delete(LAT, (delete_sample))   \n",
    "    print(\"Number of samples deleted above the 1/\"+str(fraction_of)+\" criterion = \", sample_count)\n",
    "    #print(\"VAR shape after half Col removed = \", VAR.shape)\n",
    "    #print(\"LON shape = \", LON.shape)\n",
    "    #print(\"LAT shape = \", LAT.shape)\n",
    "    return LON, LAT, VAR, VAR2, varTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dealwithNan(VAR, VAR2):\n",
    "    print(\"Load.dealwithNan\")\n",
    "    \"\"\" Simple Linear interpolator to deal with th eremaining Nan valus \"\"\"\n",
    "    print(\"total number of values before nan interpolation = \", np.size(VAR))\n",
    "    print(\"number of nans before nan interpolation = \",np.isnan(VAR).sum())\n",
    "    for i in range(np.ma.size(VAR, axis=0)):\n",
    "        mask = []\n",
    "        VAR_temp = []\n",
    "        VAR_temp = VAR[i,:]\n",
    "        mask = np.isnan(VAR_temp)\n",
    "        VAR_temp[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), VAR_temp[~mask])\n",
    "        VAR[i,:] = VAR_temp\n",
    "    \n",
    "    for i in range(np.ma.size(VAR2, axis=0)):\n",
    "        mask2 = []\n",
    "        VAR2_temp = []\n",
    "        VAR2_temp = VAR2[i,:]\n",
    "        mask2 = np.isnan(VAR2_temp)\n",
    "        VAR2_temp[mask2] = np.interp(np.flatnonzero(mask2), np.flatnonzero(~mask2), VAR2_temp[~mask2])\n",
    "        VAR2[i,:] = VAR2_temp\n",
    "    print(\"number of nans after = \",np.isnan(VAR).sum())\n",
    "    return VAR, VAR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniformTrain(lon, lat, VAR, VAR2, varTime, depth, grid, concentration):\n",
    "    print(\"Load.uniformTrain\")\n",
    "    i_array = np.arange(-180, 180, grid ,dtype=np.int32)    # array of intergers from -180 to 180\n",
    "                             # Change to 180\n",
    "    array_lon, array_lat, array_time = [], [], []\n",
    "    count = 0\n",
    "    # Loop over the Longitudinal values\n",
    "    for i in i_array:\n",
    "        indices_lon = []\n",
    "        indices_lon = np.squeeze( np.nonzero((lon>=i)&(lon<i+grid) ) )  # indices of the lon values that match the criteria i to i + grid\n",
    "        \n",
    "        lon_temp_i = lon[indices_lon] # lon values that match the criteria\n",
    "        lat_temp_i = lat[indices_lon] # lat values corresponding to lon values that match the criteria\n",
    "        time_temp_i = varTime[indices_lon] # time values corresponding to lon values that match the criteria\n",
    "        \n",
    "        if indices_lon.size == 0:\n",
    "            #print(\"Indices_lon is null for range \", i, \" + \", grid)\n",
    "            continue\n",
    "        j_array = []\n",
    "        j_array = np.arange(min(np.floor(lat[indices_lon])), max(np.ceil(lat[indices_lon])), grid ,dtype=np.int32) # array of latitude values for each longitude\n",
    "        #print(\"j_array = \", j_array)\n",
    "        \n",
    "        # Loop over the Latitudes for a given longitude\n",
    "        for j in j_array:\n",
    "            indices_lat = []\n",
    "            indices_lat = np.squeeze( np.nonzero((lat_temp_i >= j) & (lat_temp_i < j + grid)) ) # Indices of the lat values that match the criteria j to j+grid\n",
    "            \n",
    "            lon_temp_j = lon_temp_i[indices_lat] # lon values that match both lon and lat criteria\n",
    "            lat_temp_j = lat_temp_i[indices_lat] # lat values that match both lon and lat criteria\n",
    "            time_temp_j = time_temp_i[indices_lat] # time values that match both lon and lat criteria\n",
    "            #print(\"Indices: \", indices_lat.size , indices_lat)\n",
    "            \n",
    "            if indices_lat.size == 0:\n",
    "                #print(\"Indices_lat is null for range \",i , j, \" + \",grid)\n",
    "                continue\n",
    "            if indices_lat.size == 1:\n",
    "                #print(\"Converting # into [#] for indices_lat\")\n",
    "                indices_lat = np.asarray([indices_lat])\n",
    "                lon_temp_j = np.asarray([lon_temp_j])\n",
    "                lat_temp_j = np.asarray([lat_temp_j])\n",
    "                time_temp_j = np.asarray([time_temp_j])\n",
    "                \n",
    "            select = None   # Reset the select variable for every iteration\n",
    "            select = np.random.randint(indices_lat.size, size = concentration)\n",
    "            #print(i, j, \" select = \", select, \"indices_lat[select] = \", indices_lat[select])\n",
    "            \n",
    "            # Select the training sample\n",
    "            lon_select = lon_temp_j[select] # single lon value for selected sample\n",
    "            lat_select = lat_temp_j[select] # single lat value for selected sample\n",
    "            time_select = time_temp_j[select] # single time value for selected sample\n",
    "            \n",
    "            for col_depth in range(0,len(depth)):\n",
    "                variable_col, variable2_col = [], []\n",
    "                variable_col = VAR[:,col_depth]\n",
    "                variable2_col = VAR2[:,col_depth]\n",
    "                variable_train_lon = variable_col[indices_lon]\n",
    "                variable2_train_lon = variable2_col[indices_lon]\n",
    "                variable_train_lat = variable_train_lon[indices_lat]\n",
    "                variable2_train_lat = variable2_train_lon[indices_lat]\n",
    "                variable_train = variable_train_lat[select] # single depth sample selected from criteria\n",
    "                variable2_train = variable2_train_lat[select]\n",
    "                \n",
    "                if col_depth == 0:\n",
    "                    var_train_sample = variable_train.reshape(variable_train.size,1)\n",
    "                    var2_train_sample = variable2_train.reshape(variable2_train.size,1)\n",
    "                else:\n",
    "                    var_train = variable_train.reshape(variable_train.size,1)\n",
    "                    var_train_sample = np.hstack([var_train_sample, var_train])\n",
    "                    \n",
    "                    var2_train = variable2_train.reshape(variable2_train.size,1)\n",
    "                    var2_train_sample = np.hstack([var2_train_sample, var2_train])\n",
    "                    \n",
    "                # var_train_sample has whole sample selected from criteria\n",
    "\n",
    "            array_lon = np.append(array_lon,lon_select)\n",
    "            array_lat = np.append(array_lat,lat_select)\n",
    "            array_time = np.append(array_time, time_select)\n",
    "            \n",
    "            if i == i_array[0] and j == j_array[0]:\n",
    "                var_train_array = var_train_sample\n",
    "                var2_train_array = var2_train_sample\n",
    "            else:\n",
    "                var_train_array = np.vstack([var_train_array,var_train_sample])\n",
    "                var2_train_array = np.vstack([var2_train_array,var2_train_sample])\n",
    "            \n",
    "            count = count + concentration\n",
    "    \n",
    "    array_lon = np.asarray(array_lon)\n",
    "    array_lon = np.squeeze(array_lon.reshape(1,array_lon.size))\n",
    "    array_lat = np.asarray(array_lat)\n",
    "    array_lat = np.squeeze(array_lat.reshape(1,array_lat.size))\n",
    "    array_time = np.asarray(array_time)\n",
    "    array_time = np.squeeze(array_time.reshape(1,array_time.size))    \n",
    "\n",
    "    print(\"var_train_array.shape = \", var_train_array.shape)\n",
    "    \n",
    "    return array_lon, array_lat, var_train_array, var2_train_array, array_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomTrain(lon, lat, Tint, Sint, varTime, depth, fraction_train):\n",
    "    lon_rand, lat_rand, Tint_rand, Sint_rand, varTime_rand = None, None, None, None, None, None\n",
    "    array_size = np.ma.size(Tint, axis = 0)   # Expecting around 280,000\n",
    "    number_rand = fraction_train * array_size\n",
    "    \n",
    "    indices_rand = None\n",
    "    indices_rand = np.random.randint(0, high=array_size, size = number_rand)\n",
    "    \n",
    "    lon_rand = lon[indices_rand]\n",
    "    lat_rand = lat[indices_rand]\n",
    "    Tint_rand = Tint[indices_rand, :]\n",
    "    Sint_rand = Sint[indices_rand, :]\n",
    "    varTime_rand = varTime[indices_rand]\n",
    "    \n",
    "    print(\"Tint_rand.shape = \", Tint_rand.shape)\n",
    "    \n",
    "    return lon_rand, lat_rand, Tint_rand, Sint_rand, varTime_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inTimeTrain(lon, lat, Tint, Sint, varTime, depth, inTime_start, inTime_finish):\n",
    "    lon_train, lat_train, Tint_train, Sint_train, varTime_train = None, None, None, None, None\n",
    "    \n",
    "    indices_time = None\n",
    "    indices_time = np.logical_and(varTime > inTime_start, varTime < inTime_finish)\n",
    "    \n",
    "    lon_train = lon[indices_time]\n",
    "    lat_train = lat[indices_time]\n",
    "    Tint_train = Tint[indices_time, :]\n",
    "    Sint_train = Sint[indices_time, :]\n",
    "    varTime_train = varTime[indices_time]\n",
    "    \n",
    "    print(\"Tint_train.shape = \", Tint_train.shape)\n",
    "    \n",
    "    return lon_train, lat_train, Tint_train, Sint_train, varTime_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Load runtime = ', 0.30086900000000005, ' s')\n"
     ]
    }
   ],
   "source": [
    "def centreAndStandardise(address, run, VAR):\n",
    "    print(\"Load.centreAndStandardise\")\n",
    "    \"\"\" Function to creat a standardised object using the training data set \"\"\"\n",
    "    stand_store = address+\"Objects/Scale_object.pkl\"\n",
    "    stand = preprocessing.StandardScaler()\n",
    "    stand.fit(VAR)  # VAR = Tint_train so that stand is initiaised with the traing dataset\n",
    "    var_stand = stand.transform(VAR)\n",
    "    \n",
    "    with open(stand_store, 'wb') as output:\n",
    "        ScaleObject = stand\n",
    "        pickle.dump(ScaleObject, output, pickle.HIGHEST_PROTOCOL)\n",
    "    del ScaleObject\n",
    "    \n",
    "    return stand, stand_store, var_stand\n",
    "    \n",
    "\n",
    "print('Load runtime = ', time.clock() - start_time,' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
